{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/working\n!rm -R /kaggle/working/model-merging\n!rm -R /kaggle/working/models\n!mkdir -p /kaggle/working/models\n!git clone https://github.com/NiccoloSicilian/model-merging.git\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-04T22:02:33.233364Z","iopub.execute_input":"2026-02-04T22:02:33.233573Z","iopub.status.idle":"2026-02-04T22:02:34.766127Z","shell.execute_reply.started":"2026-02-04T22:02:33.233551Z","shell.execute_reply":"2026-02-04T22:02:34.765085Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\nrm: cannot remove '/kaggle/working/model-merging': No such file or directory\nrm: cannot remove '/kaggle/working/models': No such file or directory\nCloning into 'model-merging'...\nremote: Enumerating objects: 2604, done.\u001b[K\nremote: Counting objects: 100% (118/118), done.\u001b[K\nremote: Compressing objects: 100% (21/21), done.\u001b[K\nremote: Total 2604 (delta 107), reused 96 (delta 96), pack-reused 2486 (from 3)\u001b[K\nReceiving objects: 100% (2604/2604), 773.12 KiB | 6.84 MiB/s, done.\nResolving deltas: 100% (1741/1741), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 1. Ensure we are in the repo folder\n%cd /kaggle/working/model-merging\n\n# 2. Force reinstall the package in editable mode explicitly using 'uv pip'\n!uv pip install -e .\n\n# 3. Verify the installation\n# This should list 'model-merging' and its location\n!uv pip list | grep model-merging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T22:02:44.734382Z","iopub.execute_input":"2026-02-04T22:02:44.734682Z","iopub.status.idle":"2026-02-04T22:03:45.559201Z","shell.execute_reply.started":"2026-02-04T22:02:44.734652Z","shell.execute_reply":"2026-02-04T22:03:45.558277Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/model-merging\n\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m284 packages\u001b[0m \u001b[2min 3.68s\u001b[0m\u001b[0m                                       \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m181 packages\u001b[0m \u001b[2min 47.44s\u001b[0m\u001b[0m                                          \n\u001b[2mUninstalled \u001b[1m87 packages\u001b[0m \u001b[2min 7.76s\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m181 packages\u001b[0m \u001b[2min 581ms\u001b[0m\u001b[0m                             \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.9.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1maiohttp-retry\u001b[0m\u001b[2m==2.8.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.3.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1maltair\u001b[0m\u001b[2m==5.5.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1maltair\u001b[0m\u001b[2m==5.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mamqp\u001b[0m\u001b[2m==5.2.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.12.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1manypy\u001b[0m\u001b[2m==0.0.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mappdirs\u001b[0m\u001b[2m==1.4.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1marrow\u001b[0m\u001b[2m==1.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1marrow\u001b[0m\u001b[2m==1.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1masyncssh\u001b[0m\u001b[2m==2.14.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1matpublic\u001b[0m\u001b[2m==5.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1matpublic\u001b[0m\u001b[2m==4.1.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==23.2.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mbabel\u001b[0m\u001b[2m==2.17.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mbabel\u001b[0m\u001b[2m==2.14.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mbandit\u001b[0m\u001b[2m==1.7.8\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.13.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.12.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mbilliard\u001b[0m\u001b[2m==4.2.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mblack\u001b[0m\u001b[2m==25.12.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mblack\u001b[0m\u001b[2m==24.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mblessed\u001b[0m\u001b[2m==1.20.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mblinker\u001b[0m\u001b[2m==1.9.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mblinker\u001b[0m\u001b[2m==1.7.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.42.10\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.34.79\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.42.10\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.34.79\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==5.5.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==5.3.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcelery\u001b[0m\u001b[2m==5.3.6\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mcffi\u001b[0m\u001b[2m==2.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcffi\u001b[0m\u001b[2m==1.16.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcfgv\u001b[0m\u001b[2m==3.4.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.3.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mclarabel\u001b[0m\u001b[2m==0.11.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mclarabel\u001b[0m\u001b[2m==0.7.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.1.7\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mclick-didyoumean\u001b[0m\u001b[2m==0.3.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mclick-plugins\u001b[0m\u001b[2m==1.1.1.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mclick-plugins\u001b[0m\u001b[2m==1.1.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mclick-repl\u001b[0m\u001b[2m==0.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mconfigobj\u001b[0m\u001b[2m==5.0.8\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.2.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mcoverage\u001b[0m\u001b[2m==7.13.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcoverage\u001b[0m\u001b[2m==7.4.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcroniter\u001b[0m\u001b[2m==1.3.15\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==46.0.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==42.0.5\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mcvxpy\u001b[0m\u001b[2m==1.6.7\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcvxpy\u001b[0m\u001b[2m==1.4.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.4.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.2.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdateutils\u001b[0m\u001b[2m==0.6.12\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mdeepdiff\u001b[0m\u001b[2m==8.6.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdeepdiff\u001b[0m\u001b[2m==6.7.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdictdiffer\u001b[0m\u001b[2m==0.9.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdistlib\u001b[0m\u001b[2m==0.3.8\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdocker-pycreds\u001b[0m\u001b[2m==0.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdpath\u001b[0m\u001b[2m==2.1.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdulwich\u001b[0m\u001b[2m==0.21.7\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdvc\u001b[0m\u001b[2m==3.43.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdvc-data\u001b[0m\u001b[2m==3.9.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdvc-gdrive\u001b[0m\u001b[2m==3.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdvc-http\u001b[0m\u001b[2m==2.32.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdvc-objects\u001b[0m\u001b[2m==3.0.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdvc-render\u001b[0m\u001b[2m==1.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdvc-studio-client\u001b[0m\u001b[2m==0.20.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdvc-task\u001b[0m\u001b[2m==0.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mecos\u001b[0m\u001b[2m==2.0.13\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1meditor\u001b[0m\u001b[2m==1.6.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.119.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.88.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.13.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mflake8\u001b[0m\u001b[2m==7.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mflatten-dict\u001b[0m\u001b[2m==0.4.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mflufl-lock\u001b[0m\u001b[2m==7.1.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.60.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.51.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.4.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2023.12.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mftfy\u001b[0m\u001b[2m==6.2.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgeoopt\u001b[0m\u001b[2m==0.5.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgeopt\u001b[0m\u001b[2m==0.0.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mghp-import\u001b[0m\u001b[2m==2.1.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.11\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.43\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgrandalf\u001b[0m\u001b[2m==0.8\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgto\u001b[0m\u001b[2m==1.6.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mhttplib2\u001b[0m\u001b[2m==0.31.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhttplib2\u001b[0m\u001b[2m==0.22.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.27.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhydra-core\u001b[0m\u001b[2m==1.3.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhydra-submitit-launcher\u001b[0m\u001b[2m==1.2.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1midentify\u001b[0m\u001b[2m==2.5.35\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.6.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mimportlib-resources\u001b[0m\u001b[2m==6.5.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mimportlib-resources\u001b[0m\u001b[2m==6.4.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1miniconfig\u001b[0m\u001b[2m==2.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1miniconfig\u001b[0m\u001b[2m==2.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1minquirer\u001b[0m\u001b[2m==3.2.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.17.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==7.34.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==8.22.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1misort\u001b[0m\u001b[2m==5.13.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1miterative-telemetry\u001b[0m\u001b[2m==0.0.8\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mitsdangerous\u001b[0m\u001b[2m==2.2.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mitsdangerous\u001b[0m\u001b[2m==2.1.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.4.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.25.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.21.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2025.9.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2023.12.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==7.4.9\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.8.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mkombu\u001b[0m\u001b[2m==5.3.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlightning\u001b[0m\u001b[2m==2.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlightning-cloud\u001b[0m\u001b[2m==0.5.65\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mlightning-utilities\u001b[0m\u001b[2m==0.15.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlightning-utilities\u001b[0m\u001b[2m==0.11.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mmarkdown\u001b[0m\u001b[2m==3.9\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmarkdown\u001b[0m\u001b[2m==3.6\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.8.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmccabe\u001b[0m\u001b[2m==0.7.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmergedeep\u001b[0m\u001b[2m==1.3.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmkdocs\u001b[0m\u001b[2m==1.5.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmkdocs-material\u001b[0m\u001b[2m==9.5.17\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmkdocs-material-extensions\u001b[0m\u001b[2m==1.3.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmodel-merging\u001b[0m\u001b[2m==0.0.0 (from file:///kaggle/working/model-merging)\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmplcursors\u001b[0m\u001b[2m==0.7\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.0.5\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.18\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.0.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnn-template-core\u001b[0m\u001b[2m==0.3.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnodeenv\u001b[0m\u001b[2m==1.8.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopen-clip-torch\u001b[0m\u001b[2m==2.0.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mordered-set\u001b[0m\u001b[2m==4.1.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.10.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mosqp\u001b[0m\u001b[2m==1.0.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mosqp\u001b[0m\u001b[2m==0.6.5\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpaginate\u001b[0m\u001b[2m==0.5.7\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpatsy\u001b[0m\u001b[2m==1.0.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpatsy\u001b[0m\u001b[2m==0.5.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpbr\u001b[0m\u001b[2m==6.0.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.2.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==3.11.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mplotly\u001b[0m\u001b[2m==5.24.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mplotly\u001b[0m\u001b[2m==5.20.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpluggy\u001b[0m\u001b[2m==1.6.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpluggy\u001b[0m\u001b[2m==1.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpre-commit\u001b[0m\u001b[2m==3.7.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.43\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==4.25.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==5.9.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==6.1.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpycodestyle\u001b[0m\u001b[2m==2.11.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==1.10.26\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpydeck\u001b[0m\u001b[2m==0.9.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpyflakes\u001b[0m\u001b[2m==3.2.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpygit2\u001b[0m\u001b[2m==1.18.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpygit2\u001b[0m\u001b[2m==1.15.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpygtrie\u001b[0m\u001b[2m==2.5.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpymdown-extensions\u001b[0m\u001b[2m==10.20.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpyopenssl\u001b[0m\u001b[2m==25.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpyopenssl\u001b[0m\u001b[2m==24.2.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpytorch-lightning\u001b[0m\u001b[2m==2.6.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpytorch-lightning\u001b[0m\u001b[2m==2.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpytorch-lightning-bolts\u001b[0m\u001b[2m==0.3.2.post1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2024.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpyyaml-env-tag\u001b[0m\u001b[2m==0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mqdldl\u001b[0m\u001b[2m==0.1.7.post5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mreadchar\u001b[0m\u001b[2m==4.2.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.7.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mruamel-yaml\u001b[0m\u001b[2m==0.19.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mruns\u001b[0m\u001b[2m==1.3.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1ms3transfer\u001b[0m\u001b[2m==0.16.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1ms3transfer\u001b[0m\u001b[2m==0.10.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.15.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.11.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mscmrepo\u001b[0m\u001b[2m==2.1.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1msemver\u001b[0m\u001b[2m==3.0.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1msetproctitle\u001b[0m\u001b[2m==1.3.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mshortuuid\u001b[0m\u001b[2m==1.0.13\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mshtab\u001b[0m\u001b[2m==1.8.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1msqltrie\u001b[0m\u001b[2m==0.11.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.48.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.22.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mstarsessions\u001b[0m\u001b[2m==1.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mstqdm\u001b[0m\u001b[2m==0.0.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mstreamlit\u001b[0m\u001b[2m==1.52.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1msubmitit\u001b[0m\u001b[2m==1.5.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.15.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtorchmetrics\u001b[0m\u001b[2m==1.8.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtorchmetrics\u001b[0m\u001b[2m==1.0.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.39.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.20.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.12.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtypes-python-dateutil\u001b[0m\u001b[2m==2.9.0.20260124\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1muv\u001b[0m\u001b[2m==0.7.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.38.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.29.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mvine\u001b[0m\u001b[2m==5.1.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mvirtualenv\u001b[0m\u001b[2m==20.35.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mvoluptuous\u001b[0m\u001b[2m==0.16.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.22.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.16.6\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==11.0.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxmod\u001b[0m\u001b[2m==1.9.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mzc-lockfile\u001b[0m\u001b[2m==4.0\u001b[0m\n\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`ipython==8.22.0` is yanked (reason: \"Does not start with traitlets <5.12\")\u001b[0m\n\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\nmodel-merging                            0.0.0               /kaggle/working/model-merging\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n# 1. Uninstall broken packages\n!pip uninstall -y wandb numpy\n\n# 2. Install a numpy version compatible with everything (<2.0)\n# 3. Reinstall a fresh version of wandb\n!pip install \"numpy<2.0\" wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:18:08.077489Z","iopub.execute_input":"2026-02-02T19:18:08.077793Z","iopub.status.idle":"2026-02-02T19:18:18.925448Z","shell.execute_reply.started":"2026-02-02T19:18:08.077762Z","shell.execute_reply":"2026-02-02T19:18:18.924719Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: wandb 0.16.6\nUninstalling wandb-0.16.6:\n  Successfully uninstalled wandb-0.16.6\nFound existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nCollecting numpy<2.0\n  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting wandb\n  Using cached wandb-0.24.1-py3-none-manylinux_2_28_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (24.2)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.25.3)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (1.10.26)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nUsing cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\nUsing cached wandb-0.24.1-py3-none-manylinux_2_28_x86_64.whl (23.0 MB)\nInstalling collected packages: numpy, wandb\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmodel-merging 0.0.0 requires wandb==0.16.6, but you have wandb 0.24.1 which is incompatible.\ngrain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 4.25.3 which is incompatible.\nydata-profiling 4.18.0 requires jinja2<3.2,>=3.1.6, but you have jinja2 3.1.2 which is incompatible.\nydata-profiling 4.18.0 requires pydantic<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nydata-profiling 4.18.0 requires PyYAML<6.1,>=6.0.3, but you have pyyaml 6.0.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires ipykernel==6.17.1, but you have ipykernel 7.1.0 which is incompatible.\ngoogle-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.22.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\njaxlib 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.26 which is incompatible.\ntsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.88.0 which is incompatible.\ngradio 5.49.1 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.27.1 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.26 which is incompatible.\ngradio 5.49.1 requires starlette<1.0,>=0.40.0, but you have starlette 0.22.0 which is incompatible.\nalbumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.26 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nyfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\ndiffusers 0.35.2 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.27.1 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4 wandb-0.24.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%cd /kaggle/working/model-merging\n!PYTHONPATH=./src MODELS_PATH=/kaggle/working/models uv run scripts/evaluate_multitask_merging.py \\\n    core.entity=null \\\n    train.logging.logger.entity=null \\\n    train.logging.logger.mode=disabled \\\n    train.logging.logger.log_model=False merger=dual \"merger.optimal_alphas.ViT-B-16={8:1.2, 14:1.1, 20:1.2}\" nn/encoder=b16 merger.svd_compress_factor=1.0 benchmark=N14","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T22:03:45.560937Z","iopub.execute_input":"2026-02-04T22:03:45.561233Z","iopub.status.idle":"2026-02-04T22:21:15.895521Z","shell.execute_reply.started":"2026-02-04T22:03:45.561190Z","shell.execute_reply":"2026-02-04T22:21:15.894363Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/model-merging\nUsing CPython \u001b[36m3.11.7\u001b[39m\nCreating virtual environment at: \u001b[36m.venv\u001b[39m\n\u001b[2K\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`ipython==8.22.0` is yanked (reason: \"Does not start with traitlets <5.12\")\u001b[0m\n\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/285] \u001b[2mInstalling wheels...                               \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m285 packages\u001b[0m \u001b[2min 14.45s\u001b[0m\u001b[0m                            \u001b[0m\n\u001b[2;36m[22:09:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m PyTorch version \u001b[1;36m2.8\u001b[0m.\u001b[1;36m0\u001b[0m available.                \u001b]8;id=329303;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/datasets/config.py\u001b\\\u001b[2mconfig.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=468972;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/datasets/config.py#54\u001b\\\u001b[2m54\u001b[0m\u001b]8;;\u001b\\\n/kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/imports.py:14: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  import pkg_resources\n\u001b[2;36m[22:10:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m generated new fontManager               \u001b]8;id=250573;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/matplotlib/font_manager.py\u001b\\\u001b[2mfont_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=846870;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/matplotlib/font_manager.py#1578\u001b\\\u001b[2m1578\u001b[0m\u001b]8;;\u001b\\\n/kaggle/working/model-merging/src/model_merging/data/datamodule.py:206: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=str(PROJECT_ROOT / \"conf\"), config_name=\"default\")\n/kaggle/working/model-merging/scripts/evaluate_multitask_merging.py:210: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=str(PROJECT_ROOT / \"conf\"), config_name=\"multitask.yaml\")\nsys:1: UserWarning: \n'hydra/launcher/basic' is validated against ConfigStore schema with the same name.\nThis behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\nSee https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n/kaggle/working/model-merging/.venv/lib/python3.11/site-packages/hydra/main.py:94: UserWarning: \n'hydra/launcher/basic' is validated against ConfigStore schema with the same name.\nThis behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\nSee https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n  _run_hydra(\n/kaggle/working/model-merging/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\n\u001b[2;36m[22:10:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Global seed set to \u001b[1;36m1608637542\u001b[0m                     \u001b]8;id=920734;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_fabric/utilities/seed.py\u001b\\\u001b[2mseed.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=942976;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_fabric/utilities/seed.py#53\u001b\\\u001b[2m53\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Setting seed \u001b[1;36m1608637542\u001b[0m from seeds\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m           \u001b]8;id=506654;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/nn_core/common/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=125355;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/nn_core/common/utils.py#107\u001b\\\u001b[2m107\u001b[0m\u001b]8;;\u001b\\\nMERGERERERE:  {'_target_': 'model_merging.merger.dual_merging.DualMerger', 'optimal_alphas': {'ViT-B-32': {8: 1.5, 14: 1.1, 20: 1.2}, 'ViT-B-16': {8: 1.2, 14: 1.1, 20: 1.2}, 'ViT-L-14': {8: 3.0, 14: 3.8, 20: 1.2}}, 'svd_path': '${oc.env:MODELS_PATH}svd_dict_${nn.encoder.model_name}.pt', 'svd_compress_factor': 1.0, 'device': '${device}', 'model_name': '${nn.encoder.model_name}'}\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Tags: \u001b[1m[\u001b[0m\u001b[32m'dev'\u001b[0m\u001b[1m]\u001b[0m                                    \u001b]8;id=507214;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/nn_core/common/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=552910;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/nn_core/common/utils.py#96\u001b\\\u001b[2m96\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Instantiating \u001b[1m<\u001b[0m\u001b[1;95mWandbLogger\u001b[0m\u001b[1m>\u001b[0m              \u001b]8;id=346224;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/nn_core/model_logging.py\u001b\\\u001b[2mmodel_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=130475;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/nn_core/model_logging.py#41\u001b\\\u001b[2m41\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:10:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Uploading source code to W&B             \u001b]8;id=224430;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/nn_core/model_logging.py\u001b\\\u001b[2mmodel_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=170108;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/nn_core/model_logging.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:07<00:00, 59.7MB/s]\n\u001b[2;36m[22:10:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=635995;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=25814;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=591644;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=663784;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n100%|███████████████████████████████████████| 351M/351M [00:04<00:00, 73.9MiB/s]\n\u001b[2;36m[22:10:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=263442;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=560796;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:11<00:00, 37.5MB/s]\n\u001b[2;36m[22:10:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=234073;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=255163;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=507505;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=491560;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:10:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=381442;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=349771;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:07<00:00, 56.8MB/s]\n\u001b[2;36m[22:10:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=614797;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=511817;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=304382;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=8941;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:10:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=716308;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=833390;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:08<00:00, 53.9MB/s]\n\u001b[2;36m[22:11:04]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=488023;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=782401;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=581780;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=172798;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:11:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=928449;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=83169;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:07<00:00, 58.1MB/s]\n\u001b[2;36m[22:11:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=152413;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=76642;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=678117;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=717760;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:11:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=963207;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=264469;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:06<00:00, 66.3MB/s]\n\u001b[2;36m[22:11:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=462863;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=8625;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=167968;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=44657;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:11:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=921520;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=34912;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:06<00:00, 65.2MB/s]\n\u001b[2;36m[22:11:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=550522;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=749141;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=620633;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=218203;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:11:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=946741;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=876434;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:05<00:00, 78.0MB/s]\n\u001b[2;36m[22:11:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=23087;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=719696;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=280472;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=293850;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:11:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=207732;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=657915;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:07<00:00, 62.6MB/s]\n\u001b[2;36m[22:11:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=897653;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=618577;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=388272;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=244912;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:12:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=726824;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=853969;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:07<00:00, 58.5MB/s]\n\u001b[2;36m[22:12:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=6460;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=24388;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=862994;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=355988;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:12:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=669568;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=890611;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:06<00:00, 66.9MB/s]\n\u001b[2;36m[22:12:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=40315;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=640083;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=224818;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=802939;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:12:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=999769;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=591126;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:06<00:00, 64.7MB/s]\n\u001b[2;36m[22:12:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=175752;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=289571;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=109796;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=188927;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:12:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=458773;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=962066;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:07<00:00, 59.2MB/s]\n\u001b[2;36m[22:12:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=802926;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=598129;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=445466;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=176792;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:12:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=874455;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=758512;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:05<00:00, 76.4MB/s]\n\u001b[2;36m[22:12:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=126996;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=129203;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=582976;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=620599;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:12:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=241551;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=298862;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\npytorch_model.bin: 100%|█████████████████████| 447M/447M [00:07<00:00, 57.8MB/s]\n\u001b[2;36m[22:13:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=466532;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=620730;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=13047;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=354741;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m[22:13:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Removing text transformer from the model.      \u001b]8;id=531127;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=417226;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Number of tasks: \u001b[1;36m14\u001b[0m        \u001b]8;id=158601;file:///kaggle/working/model-merging/scripts/evaluate_multitask_merging.py\u001b\\\u001b[2mevaluate_multitask_merging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=990678;file:///kaggle/working/model-merging/scripts/evaluate_multitask_merging.py#117\u001b\\\u001b[2m117\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Finetuned models:          \u001b]8;id=575141;file:///kaggle/working/model-merging/scripts/evaluate_multitask_merging.py\u001b\\\u001b[2mevaluate_multitask_merging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=291264;file:///kaggle/working/model-merging/scripts/evaluate_multitask_merging.py#118\u001b\\\u001b[2m118\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m           \u001b[0m         \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:              \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'SVHN'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m4\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'???'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'_args_'\u001b[0m:                  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m[\u001b[0m\u001b[32m'ufldl-stanford/svhn'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'cropped_digits'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'MNIST'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m5\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'??'\u001b[0m,     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m: \u001b[32m'ylecun/mnist'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'SUN397'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m14\u001b[0m, \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'???'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'tanganke/sun397'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'Cars'\u001b[0m, \u001b[32m'preprocess_fn'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'???'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m35\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'tanganke/stanford_cars'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         , \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:             \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'RESISC45'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m15\u001b[0m, \u001b[32m'preprocess_fn'\u001b[0m:       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'???'\u001b[0m, \u001b[32m'hf_dataset'\u001b[0m:       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'tanganke/resisc45'\u001b[0m\u001b[1m}\u001b[0m,      \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'label_map'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;36m0\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m: \u001b[1;36m2\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m2\u001b[0m: \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[1;36m4\u001b[0m: \u001b[1;36m6\u001b[0m, \u001b[1;36m5\u001b[0m: \u001b[1;36m7\u001b[0m, \u001b[1;36m6\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m9\u001b[0m, \u001b[1;36m7\u001b[0m: \u001b[1;36m10\u001b[0m, \u001b[1;36m8\u001b[0m: \u001b[1;36m11\u001b[0m, \u001b[1;36m9\u001b[0m: \u001b[1;36m12\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m10\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[1;36m11\u001b[0m: \u001b[1;36m14\u001b[0m, \u001b[1;36m12\u001b[0m: \u001b[1;36m15\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m13\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[1;36m14\u001b[0m: \u001b[1;36m16\u001b[0m, \u001b[1;36m15\u001b[0m: \u001b[1;36m17\u001b[0m, \u001b[1;36m16\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m18\u001b[0m, \u001b[1;36m17\u001b[0m: \u001b[1;36m19\u001b[0m, \u001b[1;36m18\u001b[0m: \u001b[1;36m4\u001b[0m, \u001b[1;36m19\u001b[0m: \u001b[1;36m20\u001b[0m, \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m20\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[1;36m21\u001b[0m: \u001b[1;36m22\u001b[0m, \u001b[1;36m22\u001b[0m: \u001b[1;36m23\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m23\u001b[0m: \u001b[1;36m24\u001b[0m, \u001b[1;36m24\u001b[0m: \u001b[1;36m25\u001b[0m, \u001b[1;36m25\u001b[0m: \u001b[1;36m26\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m26\u001b[0m: \u001b[1;36m27\u001b[0m, \u001b[1;36m27\u001b[0m: \u001b[1;36m28\u001b[0m, \u001b[1;36m28\u001b[0m: \u001b[1;36m29\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m29\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[1;36m30\u001b[0m: \u001b[1;36m31\u001b[0m, \u001b[1;36m31\u001b[0m: \u001b[1;36m32\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m32\u001b[0m: \u001b[1;36m8\u001b[0m, \u001b[1;36m33\u001b[0m: \u001b[1;36m33\u001b[0m, \u001b[1;36m34\u001b[0m: \u001b[1;36m34\u001b[0m, \u001b[1;36m35\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m35\u001b[0m, \u001b[1;36m36\u001b[0m: \u001b[1;36m36\u001b[0m, \u001b[1;36m37\u001b[0m: \u001b[1;36m37\u001b[0m, \u001b[1;36m38\u001b[0m:    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m38\u001b[0m, \u001b[1;36m39\u001b[0m: \u001b[1;36m39\u001b[0m, \u001b[1;36m40\u001b[0m: \u001b[1;36m40\u001b[0m, \u001b[1;36m41\u001b[0m:    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m41\u001b[0m, \u001b[1;36m42\u001b[0m: \u001b[1;36m42\u001b[0m, \u001b[1;36m43\u001b[0m: \u001b[1;36m43\u001b[0m, \u001b[1;36m44\u001b[0m:    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m44\u001b[0m\u001b[1m}\u001b[0m,                       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'classnames_override'\u001b[0m:     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m[\u001b[0m\u001b[32m'airplane'\u001b[0m, \u001b[32m'forest'\u001b[0m,     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'airport'\u001b[0m, \u001b[32m'baseball \u001b[0m      \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mdiamond'\u001b[0m, \u001b[32m'industrial \u001b[0m     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32marea'\u001b[0m, \u001b[32m'basketball court'\u001b[0m, \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'beach'\u001b[0m, \u001b[32m'bridge'\u001b[0m,         \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'river'\u001b[0m, \u001b[32m'chaparral'\u001b[0m,      \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'church'\u001b[0m, \u001b[32m'circular \u001b[0m       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mfarmland'\u001b[0m, \u001b[32m'cloud'\u001b[0m,        \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'commercial area'\u001b[0m, \u001b[32m'dense \u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mresidential'\u001b[0m, \u001b[32m'desert'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'freeway'\u001b[0m, \u001b[32m'golf course'\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'ground track field'\u001b[0m,      \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'harbor'\u001b[0m, \u001b[32m'intersection'\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'island'\u001b[0m, \u001b[32m'lake'\u001b[0m,          \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'meadow'\u001b[0m, \u001b[32m'medium \u001b[0m         \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mresidential'\u001b[0m, \u001b[32m'mobile home\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mpark'\u001b[0m, \u001b[32m'mountain'\u001b[0m,         \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'overpass'\u001b[0m, \u001b[32m'palace'\u001b[0m,      \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'parking lot'\u001b[0m, \u001b[32m'railway'\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'railway station'\u001b[0m,         \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'rectangular farmland'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'roundabout'\u001b[0m, \u001b[32m'runway'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'sea ice'\u001b[0m, \u001b[32m'ship'\u001b[0m,         \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'snowberg'\u001b[0m, \u001b[32m'sparse \u001b[0m       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mresidential'\u001b[0m, \u001b[32m'stadium'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'storage tank'\u001b[0m, \u001b[32m'tennis \u001b[0m   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mcourt'\u001b[0m, \u001b[32m'terrace'\u001b[0m,         \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'thermal power station'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'wetland'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'EuroSAT'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m:    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m12\u001b[0m, \u001b[32m'preprocess_fn'\u001b[0m:       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'???'\u001b[0m, \u001b[32m'hf_dataset'\u001b[0m:       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'tanganke/eurosat'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,      \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'GTSRB'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m11\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'???'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'tanganke/gtsrb'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,        \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'DTD'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m76\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'???'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m: \u001b[32m'tanganke/dtd'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'Flowers102'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m147\u001b[0m, \u001b[32m'preprocess_fn'\u001b[0m:      \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'???'\u001b[0m, \u001b[32m'hf_dataset'\u001b[0m:       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'dpdl-benchmark/oxford_flo\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mwers102'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'PCAM'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m1\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'??'\u001b[0m,     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'1aurent/PatchCamelyon'\u001b[0m\u001b[1m}\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'classnames_override'\u001b[0m:     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m[\u001b[0m\u001b[32m'lymph node'\u001b[0m, \u001b[32m'lymph node\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mcontaining metastatic \u001b[0m     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mtumor tissue'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,           \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'FER2013'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m:    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m10\u001b[0m, \u001b[32m'preprocess_fn'\u001b[0m:       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'???'\u001b[0m, \u001b[32m'hf_dataset'\u001b[0m:       \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_fer2013'\u001b[0m\u001b[1m}\u001b[0m,          \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'label_map'\u001b[0m: \u001b[3;35mNone\u001b[0m,         \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'classnames_override'\u001b[0m:     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m[\u001b[0m\u001b[32m'angry'\u001b[0m, \u001b[32m'disgusted'\u001b[0m,     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'fearful'\u001b[0m, \u001b[32m'happy'\u001b[0m,        \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'neutral'\u001b[0m, \u001b[32m'sad'\u001b[0m,          \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'surprised'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,             \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'OxfordIIITPet'\u001b[0m,           \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m82\u001b[0m,           \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'???'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'timm/oxford-iiit-pet'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:               \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'STL10'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m: \u001b[1;36m6\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'???'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m: \u001b[32m'tanganke/stl10'\u001b[0m\u001b[1m}\u001b[0m, \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'label_map'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;36m0\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[1;36m2\u001b[0m: \u001b[1;36m1\u001b[0m,  \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m1\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m: \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m: \u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m7\u001b[0m, \u001b[1;36m7\u001b[0m: \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m: \u001b[1;36m9\u001b[0m, \u001b[1;36m9\u001b[0m: \u001b[1;36m10\u001b[0m\u001b[1m}\u001b[0m,     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'classnames_override'\u001b[0m:     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1m[\u001b[0m\u001b[32m'airplane'\u001b[0m, \u001b[32m'automobile'\u001b[0m, \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'bird'\u001b[0m, \u001b[32m'cat'\u001b[0m, \u001b[32m'deer'\u001b[0m,     \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'dog'\u001b[0m, \u001b[32m'frog'\u001b[0m, \u001b[32m'horse'\u001b[0m,    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'monkey'\u001b[0m, \u001b[32m'ship'\u001b[0m,          \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'truck'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m:    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'model_merging.data.datase\u001b[0m \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32mt.load_dataset'\u001b[0m, \u001b[32m'name'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'CIFAR100'\u001b[0m, \u001b[32m'ft_epochs'\u001b[0m:   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[1;36m6\u001b[0m, \u001b[32m'preprocess_fn'\u001b[0m: \u001b[32m'???'\u001b[0m, \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'hf_dataset'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'datasets.load_dataset'\u001b[0m,   \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'path'\u001b[0m:                    \u001b[2m                                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[32m'tanganke/cifar100'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m     \u001b[2m                                 \u001b[0m\n🚀 DualMerger initialized on device: cuda\n\u001b[2;36m[22:13:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using compression ratio: \u001b[1;36m1.0000\u001b[0m            \u001b]8;id=635746;file:///kaggle/working/model-merging/src/model_merging/merging/structured.py\u001b\\\u001b[2mstructured.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=306508;file:///kaggle/working/model-merging/src/model_merging/merging/structured.py#315\u001b\\\u001b[2m315\u001b[0m\u001b]8;;\u001b\\\nComputing and compressing SVD: 100%|████████████| 14/14 [01:05<00:00,  4.68s/it]\n\u001b[2;36m[22:14:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m SVD dictionary saved at:                   \u001b]8;id=441449;file:///kaggle/working/model-merging/src/model_merging/merging/structured.py\u001b\\\u001b[2mstructured.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=7579;file:///kaggle/working/model-merging/src/model_merging/merging/structured.py#333\u001b\\\u001b[2m333\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m           \u001b[0m         \u001b[35m/kaggle/working/\u001b[0m\u001b[95mmodelssvd_dict_ViT-B-16.pt\u001b[0m \u001b[2m                 \u001b[0m\n\u001b[2;36m           \u001b[0m         \u001b[95m_compress_1.0.pt\u001b[0m                           \u001b[2m                 \u001b[0m\nAveraging SVD Task Vectors: 100%|█████████████| 158/158 [00:02<00:00, 59.83it/s]\n['model.visual.conv1.weight', 'model.positional_embedding', 'model.visual.positional_embedding', 'model.visual.class_embedding', 'model.visual.transformer.resblocks.0.attn.in_proj_weight', 'model.visual.transformer.resblocks.0.attn.in_proj_bias', 'model.visual.transformer.resblocks.0.attn.out_proj.weight', 'model.visual.transformer.resblocks.0.attn.out_proj.bias', 'model.visual.transformer.resblocks.0.ln_1.weight', 'model.visual.transformer.resblocks.0.ln_1.bias', 'model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'model.visual.transformer.resblocks.0.ln_2.weight', 'model.visual.transformer.resblocks.0.ln_2.bias', 'model.visual.transformer.resblocks.1.attn.in_proj_weight', 'model.visual.transformer.resblocks.1.attn.in_proj_bias', 'model.visual.transformer.resblocks.1.attn.out_proj.weight', 'model.visual.transformer.resblocks.1.attn.out_proj.bias', 'model.visual.transformer.resblocks.1.ln_1.weight', 'model.visual.transformer.resblocks.1.ln_1.bias', 'model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'model.visual.transformer.resblocks.1.ln_2.weight', 'model.visual.transformer.resblocks.1.ln_2.bias', 'model.visual.transformer.resblocks.2.attn.in_proj_weight', 'model.visual.transformer.resblocks.2.attn.in_proj_bias', 'model.visual.transformer.resblocks.2.attn.out_proj.weight', 'model.visual.transformer.resblocks.2.attn.out_proj.bias', 'model.visual.transformer.resblocks.2.ln_1.weight', 'model.visual.transformer.resblocks.2.ln_1.bias', 'model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'model.visual.transformer.resblocks.2.ln_2.weight', 'model.visual.transformer.resblocks.2.ln_2.bias', 'model.visual.transformer.resblocks.3.attn.in_proj_weight', 'model.visual.transformer.resblocks.3.attn.in_proj_bias', 'model.visual.transformer.resblocks.3.attn.out_proj.weight', 'model.visual.transformer.resblocks.3.attn.out_proj.bias', 'model.visual.transformer.resblocks.3.ln_1.weight', 'model.visual.transformer.resblocks.3.ln_1.bias', 'model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'model.visual.transformer.resblocks.3.ln_2.weight', 'model.visual.transformer.resblocks.3.ln_2.bias', 'model.visual.transformer.resblocks.4.attn.in_proj_weight', 'model.visual.transformer.resblocks.4.attn.in_proj_bias', 'model.visual.transformer.resblocks.4.attn.out_proj.weight', 'model.visual.transformer.resblocks.4.attn.out_proj.bias', 'model.visual.transformer.resblocks.4.ln_1.weight', 'model.visual.transformer.resblocks.4.ln_1.bias', 'model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'model.visual.transformer.resblocks.4.ln_2.weight', 'model.visual.transformer.resblocks.4.ln_2.bias', 'model.visual.transformer.resblocks.5.attn.in_proj_weight', 'model.visual.transformer.resblocks.5.attn.in_proj_bias', 'model.visual.transformer.resblocks.5.attn.out_proj.weight', 'model.visual.transformer.resblocks.5.attn.out_proj.bias', 'model.visual.transformer.resblocks.5.ln_1.weight', 'model.visual.transformer.resblocks.5.ln_1.bias', 'model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'model.visual.transformer.resblocks.5.ln_2.weight', 'model.visual.transformer.resblocks.5.ln_2.bias', 'model.visual.transformer.resblocks.6.attn.in_proj_weight', 'model.visual.transformer.resblocks.6.attn.in_proj_bias', 'model.visual.transformer.resblocks.6.attn.out_proj.weight', 'model.visual.transformer.resblocks.6.attn.out_proj.bias', 'model.visual.transformer.resblocks.6.ln_1.weight', 'model.visual.transformer.resblocks.6.ln_1.bias', 'model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'model.visual.transformer.resblocks.6.ln_2.weight', 'model.visual.transformer.resblocks.6.ln_2.bias', 'model.visual.transformer.resblocks.7.attn.in_proj_weight', 'model.visual.transformer.resblocks.7.attn.in_proj_bias', 'model.visual.transformer.resblocks.7.attn.out_proj.weight', 'model.visual.transformer.resblocks.7.attn.out_proj.bias', 'model.visual.transformer.resblocks.7.ln_1.weight', 'model.visual.transformer.resblocks.7.ln_1.bias', 'model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'model.visual.transformer.resblocks.7.ln_2.weight', 'model.visual.transformer.resblocks.7.ln_2.bias', 'model.visual.transformer.resblocks.8.attn.in_proj_weight', 'model.visual.transformer.resblocks.8.attn.in_proj_bias', 'model.visual.transformer.resblocks.8.attn.out_proj.weight', 'model.visual.transformer.resblocks.8.attn.out_proj.bias', 'model.visual.transformer.resblocks.8.ln_1.weight', 'model.visual.transformer.resblocks.8.ln_1.bias', 'model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'model.visual.transformer.resblocks.8.ln_2.weight', 'model.visual.transformer.resblocks.8.ln_2.bias', 'model.visual.transformer.resblocks.9.attn.in_proj_weight', 'model.visual.transformer.resblocks.9.attn.in_proj_bias', 'model.visual.transformer.resblocks.9.attn.out_proj.weight', 'model.visual.transformer.resblocks.9.attn.out_proj.bias', 'model.visual.transformer.resblocks.9.ln_1.weight', 'model.visual.transformer.resblocks.9.ln_1.bias', 'model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'model.visual.transformer.resblocks.9.ln_2.weight', 'model.visual.transformer.resblocks.9.ln_2.bias', 'model.visual.transformer.resblocks.10.attn.in_proj_weight', 'model.visual.transformer.resblocks.10.attn.in_proj_bias', 'model.visual.transformer.resblocks.10.attn.out_proj.weight', 'model.visual.transformer.resblocks.10.attn.out_proj.bias', 'model.visual.transformer.resblocks.10.ln_1.weight', 'model.visual.transformer.resblocks.10.ln_1.bias', 'model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'model.visual.transformer.resblocks.10.ln_2.weight', 'model.visual.transformer.resblocks.10.ln_2.bias', 'model.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.visual.transformer.resblocks.11.ln_1.weight', 'model.visual.transformer.resblocks.11.ln_1.bias', 'model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.visual.transformer.resblocks.11.ln_2.weight', 'model.visual.transformer.resblocks.11.ln_2.bias', 'model.visual.ln_post.weight', 'model.visual.ln_post.bias', 'model.visual.proj', 'model.text_projection', 'model.logit_scale', 'model.visual.ln_pre.weight', 'model.visual.ln_pre.bias', 'model.token_embedding.weight', 'model.ln_final.weight', 'model.ln_final.bias']\n\n================================================================================\nSTEP 1: Creating Atomic Modules with Dualized Gradients\n================================================================================\nTOT LAYERS:  1\n✓ model.visual.conv1.weight: Conv2D [Mass: 0.5]\n⚠ model.positional_embedding: Ignored\n✓ model.visual.positional_embedding: Linear (pos emb) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.0.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.0.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.0.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.0.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.1.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.1.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.1.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.1.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.2.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.2.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.2.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.2.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.3.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.3.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.3.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.3.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.4.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.4.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.4.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.4.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.5.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.5.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.5.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.5.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.6.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.6.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.6.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.6.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.7.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.7.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.7.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.7.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.8.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.8.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.8.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.8.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.9.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.9.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.9.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.9.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.10.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.10.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.10.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.10.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.11.attn.in_proj_weight: Linear (attn in) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.11.attn.out_proj.weight: Linear (attn out) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.11.mlp.c_fc.weight: Linear (mlp fc) [Mass: 0.5]\n✓ model.visual.transformer.resblocks.11.mlp.c_proj.weight: Linear (mlp proj) [Mass: 0.5]\n✓ model.visual.proj: Linear (visual proj) [Mass: 0.5]\n⚠ model.text_projection: Ignored\n⚠ model.token_embedding.weight: Ignored\n\n================================================================================\nSTEP 2: Composing All Modules Sequentially\n================================================================================\nTotal modules to compose: 51\nComposition: model.visual.proj ∘ ... ∘ model.visual.conv1.weight\n\nStarting with: model.visual.conv1.weight [Mass: 0.50]\n  Composed: model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 1.00, Sensitivity: 1.00\n    Scalars: earlier=0.5000, later=0.5000\n  Composed: model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 1.50, Sensitivity: 1.00\n    Scalars: earlier=0.6667, later=0.3333\n  Composed: model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 2.00, Sensitivity: 1.00\n    Scalars: earlier=0.7500, later=0.2500\n  Composed: model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 2.50, Sensitivity: 1.00\n    Scalars: earlier=0.8000, later=0.2000\n  Composed: model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 3.00, Sensitivity: 1.00\n    Scalars: earlier=0.8333, later=0.1667\n  Composed: model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 3.50, Sensitivity: 1.00\n    Scalars: earlier=0.8571, later=0.1429\n  Composed: model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 4.00, Sensitivity: 1.00\n    Scalars: earlier=0.8750, later=0.1250\n  Composed: model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 4.50, Sensitivity: 1.00\n    Scalars: earlier=0.8889, later=0.1111\n  Composed: model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 5.00, Sensitivity: 1.00\n    Scalars: earlier=0.9000, later=0.1000\n  Composed: model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 5.50, Sensitivity: 1.00\n    Scalars: earlier=0.9091, later=0.0909\n  Composed: model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 6.00, Sensitivity: 1.00\n    Scalars: earlier=0.9167, later=0.0833\n  Composed: model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 6.50, Sensitivity: 1.00\n    Scalars: earlier=0.9231, later=0.0769\n  Composed: model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 7.00, Sensitivity: 1.00\n    Scalars: earlier=0.9286, later=0.0714\n  Composed: model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 7.50, Sensitivity: 1.00\n    Scalars: earlier=0.9333, later=0.0667\n  Composed: model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 8.00, Sensitivity: 1.00\n    Scalars: earlier=0.9375, later=0.0625\n  Composed: model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 8.50, Sensitivity: 1.00\n    Scalars: earlier=0.9412, later=0.0588\n  Composed: model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 9.00, Sensitivity: 1.00\n    Scalars: earlier=0.9444, later=0.0556\n  Composed: model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 9.50, Sensitivity: 1.00\n    Scalars: earlier=0.9474, later=0.0526\n  Composed: model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 10.00, Sensitivity: 1.00\n    Scalars: earlier=0.9500, later=0.0500\n  Composed: model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 10.50, Sensitivity: 1.00\n    Scalars: earlier=0.9524, later=0.0476\n  Composed: model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 11.00, Sensitivity: 1.00\n    Scalars: earlier=0.9545, later=0.0455\n  Composed: model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 11.50, Sensitivity: 1.00\n    Scalars: earlier=0.9565, later=0.0435\n  Composed: model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 12.00, Sensitivity: 1.00\n    Scalars: earlier=0.9583, later=0.0417\n  Composed: model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 12.50, Sensitivity: 1.00\n    Scalars: earlier=0.9600, later=0.0400\n  Composed: model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 13.00, Sensitivity: 1.00\n    Scalars: earlier=0.9615, later=0.0385\n  Composed: model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 13.50, Sensitivity: 1.00\n    Scalars: earlier=0.9630, later=0.0370\n  Composed: model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 14.00, Sensitivity: 1.00\n    Scalars: earlier=0.9643, later=0.0357\n  Composed: model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 14.50, Sensitivity: 1.00\n    Scalars: earlier=0.9655, later=0.0345\n  Composed: model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 15.00, Sensitivity: 1.00\n    Scalars: earlier=0.9667, later=0.0333\n  Composed: model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 15.50, Sensitivity: 1.00\n    Scalars: earlier=0.9677, later=0.0323\n  Composed: model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 16.00, Sensitivity: 1.00\n    Scalars: earlier=0.9688, later=0.0312\n  Composed: model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 16.50, Sensitivity: 1.00\n    Scalars: earlier=0.9697, later=0.0303\n  Composed: model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 17.00, Sensitivity: 1.00\n    Scalars: earlier=0.9706, later=0.0294\n  Composed: model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 17.50, Sensitivity: 1.00\n    Scalars: earlier=0.9714, later=0.0286\n  Composed: model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 18.00, Sensitivity: 1.00\n    Scalars: earlier=0.9722, later=0.0278\n  Composed: model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 18.50, Sensitivity: 1.00\n    Scalars: earlier=0.9730, later=0.0270\n  Composed: model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 19.00, Sensitivity: 1.00\n    Scalars: earlier=0.9737, later=0.0263\n  Composed: model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 19.50, Sensitivity: 1.00\n    Scalars: earlier=0.9744, later=0.0256\n  Composed: model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 20.00, Sensitivity: 1.00\n    Scalars: earlier=0.9750, later=0.0250\n  Composed: model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 20.50, Sensitivity: 1.00\n    Scalars: earlier=0.9756, later=0.0244\n  Composed: model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 21.00, Sensitivity: 1.00\n    Scalars: earlier=0.9762, later=0.0238\n  Composed: model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 21.50, Sensitivity: 1.00\n    Scalars: earlier=0.9767, later=0.0233\n  Composed: model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 22.00, Sensitivity: 1.00\n    Scalars: earlier=0.9773, later=0.0227\n  Composed: model.visual.transformer.resblocks.10.mlp.c_fc.weight∘model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 22.50, Sensitivity: 1.00\n    Scalars: earlier=0.9778, later=0.0222\n  Composed: model.visual.transformer.resblocks.10.mlp.c_proj.weight∘model.visual.transformer.resblocks.10.mlp.c_fc.weight∘model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 23.00, Sensitivity: 1.00\n    Scalars: earlier=0.9783, later=0.0217\n  Composed: model.visual.transformer.resblocks.11.attn.in_proj_weight∘model.visual.transformer.resblocks.10.mlp.c_proj.weight∘model.visual.transformer.resblocks.10.mlp.c_fc.weight∘model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 23.50, Sensitivity: 1.00\n    Scalars: earlier=0.9787, later=0.0213\n  Composed: model.visual.transformer.resblocks.11.attn.out_proj.weight∘model.visual.transformer.resblocks.11.attn.in_proj_weight∘model.visual.transformer.resblocks.10.mlp.c_proj.weight∘model.visual.transformer.resblocks.10.mlp.c_fc.weight∘model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 24.00, Sensitivity: 1.00\n    Scalars: earlier=0.9792, later=0.0208\n  Composed: model.visual.transformer.resblocks.11.mlp.c_fc.weight∘model.visual.transformer.resblocks.11.attn.out_proj.weight∘model.visual.transformer.resblocks.11.attn.in_proj_weight∘model.visual.transformer.resblocks.10.mlp.c_proj.weight∘model.visual.transformer.resblocks.10.mlp.c_fc.weight∘model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 24.50, Sensitivity: 1.00\n    Scalars: earlier=0.9796, later=0.0204\n  Composed: model.visual.transformer.resblocks.11.mlp.c_proj.weight∘model.visual.transformer.resblocks.11.mlp.c_fc.weight∘model.visual.transformer.resblocks.11.attn.out_proj.weight∘model.visual.transformer.resblocks.11.attn.in_proj_weight∘model.visual.transformer.resblocks.10.mlp.c_proj.weight∘model.visual.transformer.resblocks.10.mlp.c_fc.weight∘model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 25.00, Sensitivity: 1.00\n    Scalars: earlier=0.9800, later=0.0200\n  Composed: model.visual.proj∘model.visual.transformer.resblocks.11.mlp.c_proj.weight∘model.visual.transformer.resblocks.11.mlp.c_fc.weight∘model.visual.transformer.resblocks.11.attn.out_proj.weight∘model.visual.transformer.resblocks.11.attn.in_proj_weight∘model.visual.transformer.resblocks.10.mlp.c_proj.weight∘model.visual.transformer.resblocks.10.mlp.c_fc.weight∘model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\n    Mass: 25.50, Sensitivity: 1.00\n    Scalars: earlier=0.9804, later=0.0196\n\n================================================================================\nFINAL COMPOSED MODULE\n================================================================================\nName:        model.visual.proj∘model.visual.transformer.resblocks.11.mlp.c_proj.weight∘model.visual.transformer.resblocks.11.mlp.c_fc.weight∘model.visual.transformer.resblocks.11.attn.out_proj.weight∘model.visual.transformer.resblocks.11.attn.in_proj_weight∘model.visual.transformer.resblocks.10.mlp.c_proj.weight∘model.visual.transformer.resblocks.10.mlp.c_fc.weight∘model.visual.transformer.resblocks.10.attn.out_proj.weight∘model.visual.transformer.resblocks.10.attn.in_proj_weight∘model.visual.transformer.resblocks.9.mlp.c_proj.weight∘model.visual.transformer.resblocks.9.mlp.c_fc.weight∘model.visual.transformer.resblocks.9.attn.out_proj.weight∘model.visual.transformer.resblocks.9.attn.in_proj_weight∘model.visual.transformer.resblocks.8.mlp.c_proj.weight∘model.visual.transformer.resblocks.8.mlp.c_fc.weight∘model.visual.transformer.resblocks.8.attn.out_proj.weight∘model.visual.transformer.resblocks.8.attn.in_proj_weight∘model.visual.transformer.resblocks.7.mlp.c_proj.weight∘model.visual.transformer.resblocks.7.mlp.c_fc.weight∘model.visual.transformer.resblocks.7.attn.out_proj.weight∘model.visual.transformer.resblocks.7.attn.in_proj_weight∘model.visual.transformer.resblocks.6.mlp.c_proj.weight∘model.visual.transformer.resblocks.6.mlp.c_fc.weight∘model.visual.transformer.resblocks.6.attn.out_proj.weight∘model.visual.transformer.resblocks.6.attn.in_proj_weight∘model.visual.transformer.resblocks.5.mlp.c_proj.weight∘model.visual.transformer.resblocks.5.mlp.c_fc.weight∘model.visual.transformer.resblocks.5.attn.out_proj.weight∘model.visual.transformer.resblocks.5.attn.in_proj_weight∘model.visual.transformer.resblocks.4.mlp.c_proj.weight∘model.visual.transformer.resblocks.4.mlp.c_fc.weight∘model.visual.transformer.resblocks.4.attn.out_proj.weight∘model.visual.transformer.resblocks.4.attn.in_proj_weight∘model.visual.transformer.resblocks.3.mlp.c_proj.weight∘model.visual.transformer.resblocks.3.mlp.c_fc.weight∘model.visual.transformer.resblocks.3.attn.out_proj.weight∘model.visual.transformer.resblocks.3.attn.in_proj_weight∘model.visual.transformer.resblocks.2.mlp.c_proj.weight∘model.visual.transformer.resblocks.2.mlp.c_fc.weight∘model.visual.transformer.resblocks.2.attn.out_proj.weight∘model.visual.transformer.resblocks.2.attn.in_proj_weight∘model.visual.transformer.resblocks.1.mlp.c_proj.weight∘model.visual.transformer.resblocks.1.mlp.c_fc.weight∘model.visual.transformer.resblocks.1.attn.out_proj.weight∘model.visual.transformer.resblocks.1.attn.in_proj_weight∘model.visual.transformer.resblocks.0.mlp.c_proj.weight∘model.visual.transformer.resblocks.0.mlp.c_fc.weight∘model.visual.transformer.resblocks.0.attn.out_proj.weight∘model.visual.transformer.resblocks.0.attn.in_proj_weight∘model.visual.positional_embedding∘model.visual.conv1.weight\nTotal Mass:  25.50\nSensitivity: 1.00\nGradients:   51 layers\n================================================================================\n\nUSING ALPHA: 1.1\n\u001b[2;36m[22:14:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m before eval -- memory in MB: \u001b[1;36m14931.9453125\u001b[0m       \u001b]8;id=461447;file:///kaggle/working/model-merging/src/model_merging/utils/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=712102;file:///kaggle/working/model-merging/src/model_merging/utils/utils.py#23\u001b\\\u001b[2m23\u001b[0m\u001b]8;;\u001b\\\nREADME.md: 10.5kB [00:00, 20.0MB/s]\ntrain-00000-of-00001.parquet: 100%|██████████| 136M/136M [00:19<00:00, 7.06MB/s]\ntest-00000-of-00001.parquet: 100%|█████████| 47.0M/47.0M [00:15<00:00, 3.08MB/s]\nextra-00000-of-00002.parquet: 100%|███████████| 511M/511M [00:04<00:00, 107MB/s]\nextra-00001-of-00002.parquet: 100%|███████████| 512M/512M [00:04<00:00, 103MB/s]\nGenerating train split: 100%|██| 73257/73257 [00:00<00:00, 134936.73 examples/s]\nGenerating test split: 100%|███| 26032/26032 [00:00<00:00, 178987.19 examples/s]\nGenerating extra split: 100%|█| 531131/531131 [00:03<00:00, 175543.99 examples/s\n/kaggle/working/model-merging/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\nLoading classification head from /kaggle/working/models/ViT-B-16/head_SVHN.pt\n\u001b[2;36m[22:15:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Building classification head for SVHN           \u001b]8;id=417222;file:///kaggle/working/model-merging/src/model_merging/model/heads.py\u001b\\\u001b[2mheads.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=714568;file:///kaggle/working/model-merging/src/model_merging/model/heads.py#135\u001b\\\u001b[2m135\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=878282;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=623063;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=245583;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=822971;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n100%|████████████████████████████████████████| 351M/351M [00:01<00:00, 240MiB/s]\nBuilding classification head.\n100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 18.42it/s]\nSaving classification head to /kaggle/working/models/ViT-B-16/head_SVHN.pt\n/kaggle/working/model-merging/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n  rank_zero_warn(\n/kaggle/working/model-merging/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'classifier' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['classifier'])`.\n  rank_zero_warn(\nINFO: GPU available: True (cuda), used: True\n\u001b[2;36m[22:15:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m GPU available: \u001b[3;92mTrue\u001b[0m \u001b[1m(\u001b[0mcuda\u001b[1m)\u001b[0m, used: \u001b[3;92mTrue\u001b[0m       \u001b]8;id=323130;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=279501;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\nINFO: TPU available: False, using: 0 TPU cores\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TPU available: \u001b[3;91mFalse\u001b[0m, using: \u001b[1;36m0\u001b[0m TPU cores     \u001b]8;id=718687;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=34987;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\nINFO: IPU available: False, using: 0 IPUs\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m IPU available: \u001b[3;91mFalse\u001b[0m, using: \u001b[1;36m0\u001b[0m IPUs          \u001b]8;id=977307;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=17333;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\nINFO: HPU available: False, using: 0 HPUs\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HPU available: \u001b[3;91mFalse\u001b[0m, using: \u001b[1;36m0\u001b[0m HPUs          \u001b]8;id=507057;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=684225;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\nINFO: `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m `\u001b[1;35mTrainer\u001b[0m\u001b[1m(\u001b[0m\u001b[33mval_check_interval\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1m)\u001b[0m` was        \u001b]8;id=962866;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=689737;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m           \u001b[0m         configured so validation will run at the end \u001b[2m               \u001b[0m\n\u001b[2;36m           \u001b[0m         of the training epoch..                      \u001b[2m               \u001b[0m\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Evaluating on the SVHN     \u001b]8;id=529599;file:///kaggle/working/model-merging/scripts/evaluate_multitask_merging.py\u001b\\\u001b[2mevaluate_multitask_merging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=500584;file:///kaggle/working/model-merging/scripts/evaluate_multitask_merging.py#174\u001b\\\u001b[2m174\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m           \u001b[0m         test set!                  \u001b[2m                                 \u001b[0m\n\u001b[2;36m[22:15:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOCAL_RANK: \u001b[1;36m0\u001b[0m - CUDA_VISIBLE_DEVICES: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m,\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m       \u001b]8;id=328509;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/pytorch_lightning/accelerators/cuda.py\u001b\\\u001b[2mcuda.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=581126;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/pytorch_lightning/accelerators/cuda.py#57\u001b\\\u001b[2m57\u001b[0m\u001b]8;;\u001b\\\nTesting DataLoader 0: 100%|███████████████████| 204/204 [05:16<00:00,  1.55s/it]\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m      acc/test/SVHN      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8707360029220581    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m     loss/test/SVHN      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4748493432998657    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36mnormalized_acc/test/SVHN \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8931751847267151    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\nREADME.md: 6.97kB [00:00, 18.2MB/s]\ntrain-00000-of-00001.parquet: 100%|████████| 15.6M/15.6M [00:05<00:00, 2.67MB/s]\ntest-00000-of-00001.parquet: 100%|█████████| 2.60M/2.60M [00:00<00:00, 3.51MB/s]\nGenerating train split: 100%|██| 60000/60000 [00:00<00:00, 255690.48 examples/s]\nGenerating test split: 100%|███| 10000/10000 [00:00<00:00, 239142.93 examples/s]\nLoading classification head from /kaggle/working/models/ViT-B-16/head_MNIST.pt\n\u001b[2;36m[22:21:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Building classification head for MNIST          \u001b]8;id=904594;file:///kaggle/working/model-merging/src/model_merging/model/heads.py\u001b\\\u001b[2mheads.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=97628;file:///kaggle/working/model-merging/src/model_merging/model/heads.py#135\u001b\\\u001b[2m135\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading ViT-B-\u001b[1;36m16\u001b[0m pre-trained weights.          \u001b]8;id=737522;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py\u001b\\\u001b[2mencoder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=726758;file:///kaggle/working/model-merging/src/model_merging/model/encoder.py#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.       \u001b]8;id=98910;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mfactory.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=438937;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\nBuilding classification head.\n100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 114.15it/s]\nSaving classification head to /kaggle/working/models/ViT-B-16/head_MNIST.pt\nINFO: GPU available: True (cuda), used: True\n\u001b[2;36m[22:21:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m GPU available: \u001b[3;92mTrue\u001b[0m \u001b[1m(\u001b[0mcuda\u001b[1m)\u001b[0m, used: \u001b[3;92mTrue\u001b[0m       \u001b]8;id=982290;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=4140;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\nINFO: TPU available: False, using: 0 TPU cores\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TPU available: \u001b[3;91mFalse\u001b[0m, using: \u001b[1;36m0\u001b[0m TPU cores     \u001b]8;id=841728;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=261534;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\nINFO: IPU available: False, using: 0 IPUs\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m IPU available: \u001b[3;91mFalse\u001b[0m, using: \u001b[1;36m0\u001b[0m IPUs          \u001b]8;id=623927;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=472371;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\nINFO: HPU available: False, using: 0 HPUs\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HPU available: \u001b[3;91mFalse\u001b[0m, using: \u001b[1;36m0\u001b[0m HPUs          \u001b]8;id=449495;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=307179;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\nINFO: `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m `\u001b[1;35mTrainer\u001b[0m\u001b[1m(\u001b[0m\u001b[33mval_check_interval\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1m)\u001b[0m` was        \u001b]8;id=465123;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py\u001b\\\u001b[2mrank_zero.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=679478;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m           \u001b[0m         configured so validation will run at the end \u001b[2m               \u001b[0m\n\u001b[2;36m           \u001b[0m         of the training epoch..                      \u001b[2m               \u001b[0m\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Evaluating on the MNIST    \u001b]8;id=684341;file:///kaggle/working/model-merging/scripts/evaluate_multitask_merging.py\u001b\\\u001b[2mevaluate_multitask_merging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=24358;file:///kaggle/working/model-merging/scripts/evaluate_multitask_merging.py#174\u001b\\\u001b[2m174\u001b[0m\u001b]8;;\u001b\\\n\u001b[2;36m           \u001b[0m         test set!                  \u001b[2m                                 \u001b[0m\n\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOCAL_RANK: \u001b[1;36m0\u001b[0m - CUDA_VISIBLE_DEVICES: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m,\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m       \u001b]8;id=110859;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/pytorch_lightning/accelerators/cuda.py\u001b\\\u001b[2mcuda.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=272605;file:///kaggle/working/model-merging/.venv/lib/python3.11/site-packages/pytorch_lightning/accelerators/cuda.py#57\u001b\\\u001b[2m57\u001b[0m\u001b]8;;\u001b\\\n^C\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ae02f16e0c0>\nTraceback (most recent call last):\n  File \"/kaggle/working/model-merging/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/kaggle/working/model-merging/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n  File \"/root/.local/share/uv/python/cpython-3.11.7-linux-x86_64-gnu/lib/python3.11/multiprocessing/process.py\", line 149, in join\n    res = self._popen.wait(timeout)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/share/uv/python/cpython-3.11.7-linux-x86_64-gnu/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n    if not wait([self.sentinel], timeout):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/share/uv/python/cpython-3.11.7-linux-x86_64-gnu/lib/python3.11/multiprocessing/connection.py\", line 947, in wait\n    ready = selector.select(timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/share/uv/python/cpython-3.11.7-linux-x86_64-gnu/lib/python3.11/selectors.py\", line 415, in select\n    fd_event_list = self._selector.poll(timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt: \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"svd_compress_factor of the models = null (next set to 1, no compression)\nMINE alpha 1.3 (compress factor 1/num_task):\n**ViT-32 8 tasks**\n| Dataset     | THEIRS Acc | THEIRS Loss | THEIRS Norm Acc |   MINE Acc | MINE Loss | MINE Norm Acc |\n|:------------|-----------:|------------:|----------------:|-----------:|----------:|--------------:|\n| SUN397      |     0.7028 |      1.1064 |          0.9386 |     0.7049 |    1.0770 |        0.9414 |\n| Cars        |     0.7415 |      0.8322 |          0.9289 |     0.7474 |    0.7984 |        0.9364 |\n| RESISC45    |     0.8581 |      0.5138 |          0.9004 |     0.8849 |    0.4150 |        0.9285 |\n| EuroSAT     |     0.9578 |      0.1511 |          0.9649 |     0.9611 |    0.1323 |        0.9683 |\n| SVHN        |     0.8926 |      0.3810 |          0.9233 |     0.8756 |    0.4214 |        0.9058 |\n| GTSRB       |     0.8943 |      0.4043 |          0.9040 |     0.9349 |    0.2608 |        0.9450 |\n| MNIST       |     0.9871 |      0.0816 |          0.9926 |     0.9857 |    0.0856 |        0.9912 |\n| DTD         |     0.6606 |      1.2684 |          0.8449 |     0.6995 |    1.0936 |        0.8946 |\n| **Average** | **0.8368** |           — |      **0.9247** | **0.8493** |         — |    **0.9389** |\n\nMINE alpha 1.3 (compress factor 1/num_task):\n**ViT-32 14 tasks**\n| Dataset       | THEIRS Acc | THEIRS Loss | THEIRS Norm Acc |   MINE Acc | MINE Loss | MINE Norm Acc |\n| ------------- | ---------: | ----------: | --------------: | ---------: | --------: | ------------: |\n| SUN397        |     0.6898 |      1.0703 |          0.9212 |     0.6887 |    1.1076 |        0.9197 |\n| Cars          |     0.6847 |      0.9641 |          0.8579 |     0.6990 |    0.9421 |        0.8758 |\n| RESISC45      |     0.8284 |      0.5694 |          0.8693 |     0.8349 |    0.5671 |        0.8761 |\n| EuroSAT       |     0.9022 |      0.2774 |          0.9090 |     0.9181 |    0.2344 |        0.9250 |\n| SVHN          |     0.6684 |      1.0469 |          0.6915 |     0.7730 |    0.7187 |        0.7997 |\n| GTSRB         |     0.7651 |      0.8257 |          0.7733 |     0.8673 |    0.4970 |        0.8767 |\n| MNIST         |     0.9364 |      0.3433 |          0.9416 |     0.9725 |    0.1546 |        0.9779 |\n| DTD           |     0.6436 |      1.2789 |          0.8231 |     0.6766 |    1.2038 |        0.8653 |\n| Flowers102    |     0.8270 |      0.7043 |          0.9202 |     0.8213 |    0.6980 |        0.9139 |\n| PCAM          |     0.7516 |      0.5281 |          0.8713 |     0.8058 |    0.4459 |        0.9342 |\n| FER2013       |     0.6279 |      1.0661 |          0.8808 |     0.6535 |    0.9793 |        0.9167 |\n| OxfordIIITPet |     0.9166 |      0.2633 |          0.9888 |     0.9076 |    0.3167 |        0.9791 |\n| STL10         |     0.9800 |      0.0677 |          1.0046 |     0.9783 |    0.0824 |        1.0028 |\n| CIFAR100      |     0.7513 |      0.9090 |          0.8640 |     0.7306 |    1.0281 |        0.8402 |\n| **Average**   |     0.7838 |           — |      **0.8798** |     0.8091 |         — |    **0.9074** |\n\n\nMINE alpha 1.2 (compress factor 1/num_task)\n**ViT-32 20 tasks**\n\n| Dataset       |    Acc | Loss   |   Norm Acc |\n|:--------------|-------:|:-------|-----------:|\n| SUN397        | 0.6767 | 1.1291 |     0.9038 |\n| Cars          | 0.6418 | 1.1482 |     0.8041 |\n| RESISC45      | 0.8013 | 0.6612 |     0.8408 |\n| EuroSAT       | 0.8885 | 0.3190 |     0.8951 |\n| SVHN          | 0.6891 | 0.9639 |     0.7128 |\n| GTSRB         | 0.7797 | 0.7833 |     0.7881 |\n| MNIST         | 0.9767 | 0.1600 |     0.9821 |\n| DTD           | 0.6309 | 1.3314 |     0.8068 |\n| Flowers102    | 0.8045 | 0.7583 |     0.8952 |\n| PCAM          | 0.7749 | 0.5020 |     0.8984 |\n| FER2013       | 0.6351 | 1.0488 |     0.8910 |\n| OxfordIIITPet | 0.9022 | 0.3177 |     0.9732 |\n| STL10         | 0.9731 | 0.1003 |     0.9976 |\n| CIFAR100      | 0.7159 | 1.0814 |     0.8233 |\n| CIFAR10       | 0.9466 | 0.1770 |     0.9706 |\n| Food101       | 0.8300 | 0.6003 |     0.9647 |\n| FashionMNIST  | 0.8090 | 0.5858 |     0.8592 |\n| EMNIST        | 0.9808 | 0.1336 |     0.9862 |\n| KMNIST        | 0.2941 | 2.1416 |     0.2997 |\n| RenderedSST2  | 0.6930 | 0.5863 |     1.0210 |\n| **avg** | **0.7722** | **—** | **0.8657** |\n\n\nMINE alpha 1.3 (no compression):\n| Dataset       |    Acc | Loss   |   Norm Acc |\n|:--------------|-------:|:-------|-----------:|\n| SUN397        | 0.7007 | 1.0766 |     0.9358 |\n| Cars          | 0.7086 | 0.9156 |     0.8878 |\n| RESISC45      | 0.7887 | 0.7211 |     0.8276 |\n| EuroSAT       | 0.8933 | 0.2975 |     0.9    |\n| SVHN          | 0.8051 | 0.6283 |     0.8329 |\n| GTSRB         | 0.7939 | 0.7224 |     0.8025 |\n| MNIST         | 0.9731 | 0.1418 |     0.9785 |\n| DTD           | 0.6133 | 1.4148 |     0.7844 |\n| Flowers102    | 0.7232 | 1.2193 |     0.8047 |\n| PCAM          | 0.8188 | 0.4215 |     0.9492 |\n| FER2013       | 0.6585 | 0.9767 |     0.9238 |\n| OxfordIIITPet | 0.8921 | 0.3421 |     0.9624 |\n| STL10         | 0.9697 | 0.1075 |     0.9941 |\n| CIFAR100      | 0.7692 | 0.8778 |     0.8845 |\n| avg           | 0.7935 | —      |     0.8906 |\n\n\nMINE alpha 1.0 (no compression):\n| Dataset | Acc | Loss | Norm Acc |\n|--------|----:|-----:|---------:|\n| SUN397 | 0.7057 | 1.0279 | 0.9425 |\n| Cars | 0.7076 | 0.8965 | 0.8866 |\n| RESISC45 | 0.7938 | 0.6678 | 0.8329 |\n| EuroSAT | 0.8793 | 0.3229 | 0.8858 |\n| SVHN | 0.7434 | 0.8150 | 0.7690 |\n| GTSRB | 0.7317 | 0.9089 | 0.7396 |\n| MNIST | 0.9534 | 0.2591 | 0.9587 |\n| DTD | 0.6027 | 1.4361 | 0.7707 |\n| Flowers102 | 0.7400 | 1.1358 | 0.8234 |\n| PCAM | 0.8115 | 0.4597 | 0.9407 |\n| FER2013 | 0.6495 | 1.0192 | 0.9111 |\n| OxfordIIITPet | 0.8997 | 0.3138 | 0.9706 |\n| STL10 | 0.9745 | 0.0899 | 0.9990 |\n| CIFAR100 | 0.7762 | 0.8158 | 0.8926 |\n| avg | 0.7835 | — | 0.8802 |\n\n\nMINE alpha 1.2 (no compression):\n| Dataset | Acc | Loss | Norm Acc |\n|--------|----:|-----:|---------:|\n| SUN397 | 0.7048 | 1.0528 | 0.9413 |\n| Cars | 0.7094 | 0.9029 | 0.8888 |\n| RESISC45 | 0.7921 | 0.6934 | 0.8311 |\n| EuroSAT | 0.8911 | 0.3004 | 0.8978 |\n| SVHN | 0.7881 | 0.6810 | 0.8153 |\n| GTSRB | 0.7785 | 0.7743 | 0.7869 |\n| MNIST | 0.9688 | 0.1711 | 0.9742 |\n| DTD | 0.6144 | 1.4121 | 0.7857 |\n| Flowers102 | 0.7313 | 1.1802 | 0.8138 |\n| PCAM | 0.8215 | 0.4306 | 0.9523 |\n| FER2013 | 0.6576 | 0.9854 | 0.9224 |\n| OxfordIIITPet | 0.8992 | 0.3277 | 0.9700 |\n| STL10 | 0.9721 | 0.1004 | 0.9965 |\n| CIFAR100 | 0.7735 | 0.8493 | 0.8895 |\n| avg | 0.7930 | — | 0.8904 |\n\nMINE alpha 1.5 (no compression):\n| Dataset | Acc | Loss | Norm Acc |\n|--------|----:|-----:|---------:|\n| SUN397 | 0.6907 | 1.1486 | 0.9224 |\n| Cars | 0.7026 | 0.9599 | 0.8803 |\n| RESISC45 | 0.7741 | 0.8048 | 0.8123 |\n| EuroSAT | 0.8981 | 0.3041 | 0.9049 |\n| SVHN | 0.8312 | 0.5462 | 0.8598 |\n| GTSRB | 0.8164 | 0.6458 | 0.8252 |\n| MNIST | 0.9806 | 0.1021 | 0.9860 |\n| DTD | 0.6053 | 1.4496 | 0.7741 |\n| Flowers102 | 0.6972 | 1.3343 | 0.7758 |\n| PCAM | 0.8095 | 0.4154 | 0.9385 |\n| FER2013 | 0.6601 | 0.9765 | 0.9259 |\n| OxfordIIITPet | 0.8817 | 0.3854 | 0.9512 |\n| STL10 | 0.9631 | 0.1268 | 0.9873 |\n| CIFAR100 | 0.7604 | 0.9574 | 0.8744 |\n| avg | 0.7908 | — | 0.8870 |\n\nMINE alpha 1.4 (compression): \n| Dataset | Acc | Loss | Norm Acc |\n|--------|----:|-----:|---------:|\n| SUN397 | 0.6852 | 1.1309 | 0.9152 |\n| Cars | 0.6976 | 0.9557 | 0.8739 |\n| RESISC45 | 0.8275 | 0.5888 | 0.8683 |\n| EuroSAT | 0.9219 | 0.2327 | 0.9287 |\n| SVHN | 0.7905 | 0.6651 | 0.8178 |\n| GTSRB | 0.8779 | 0.4489 | 0.8874 |\n| MNIST | 0.9758 | 0.1288 | 0.9812 |\n| DTD | 0.6766 | 1.2070 | 0.8653 |\n| Flowers102 | 0.8118 | 0.7372 | 0.9034 |\n| PCAM | 0.8102 | 0.4348 | 0.9393 |\n| FER2013 | 0.6566 | 0.9694 | 0.9210 |\n| OxfordIIITPet | 0.9035 | 0.3432 | 0.9747 |\n| STL10 | 0.9762 | 0.0887 | 1.0008 |\n| CIFAR100 | 0.7227 | 1.0731 | 0.8311 |\n| avg | 0.8096 | — | 0.9077 |\n\nMINE alpha 1.8 compression: 0.89","metadata":{}},{"cell_type":"code","source":"\nimport ast\nimport re\n\nraw_str = \"\"\"\n{'SUN397':               \n                    [{'acc/test/SUN397':                                        \n                    0.685239315032959,                                          \n                    'loss/test/SUN397':                                         \n                    1.1308931112289429,                                         \n                    'normalized_acc/test/SUN39                                  \n                    7': 0.9151584506034851}],                                   \n                    'Cars': [{'acc/test/Cars':                                  \n                    0.6975500583648682,                                         \n                    'loss/test/Cars':                                           \n                    0.9557216167449951,                                         \n                    'normalized_acc/test/Cars'                                  \n                    : 0.8739482760429382}],                                     \n                    'RESISC45':                                                 \n                    [{'acc/test/RESISC45':                                      \n                    0.8274602890014648,                                         \n                    'loss/test/RESISC45':                                       \n                    0.5887748003005981,                                         \n                    'normalized_acc/test/RESIS                                  \n                    C45':                                                       \n                    0.8682544827461243}],                                       \n                    'EuroSAT':                                                  \n                    [{'acc/test/EuroSAT':                                       \n                    0.9218518733978271,                                         \n                    'loss/test/EuroSAT':                                        \n                    0.23271134495735168,                                        \n                    'normalized_acc/test/EuroS                                  \n                    AT': 0.928731381893158}],                                   \n                    'SVHN': [{'acc/test/SVHN':                                  \n                    0.790488600730896,                                          \n                    'loss/test/SVHN':                                           \n                    0.6651220321655273,                                         \n                    'normalized_acc/test/SVHN'                                  \n                    : 0.8177554607391357}],                                     \n                    'GTSRB':                                                    \n                    [{'acc/test/GTSRB':                                         \n                    0.8779097199440002,                                         \n                    'loss/test/GTSRB':                                          \n                    0.4489012062549591,                                         \n                    'normalized_acc/test/GTSRB                                  \n                    ': 0.8873949646949768}],                                    \n                    'MNIST':                                                    \n                    [{'acc/test/MNIST':                                         \n                    0.9757999777793884,                                         \n                    'loss/test/MNIST':                                          \n                    0.12883076071739197,                                        \n                    'normalized_acc/test/MNIST                                  \n                    ': 0.9811965823173523}],                                    \n                    'DTD': [{'acc/test/DTD':                                    \n                    0.6765957474708557,                                         \n                    'loss/test/DTD':                                            \n                    1.2070364952087402,                                         \n                    'normalized_acc/test/DTD':                                  \n                    0.8653061389923096}],                                       \n                    'Flowers102':                                               \n                    [{'acc/test/Flowers102':                                    \n                    0.8118393421173096,                                         \n                    'loss/test/Flowers102':                                     \n                    0.7372411489486694,                                         \n                    'normalized_acc/test/Flowe                                  \n                    rs102':                                                     \n                    0.9033659100532532}],                                       \n                    'PCAM': [{'acc/test/PCAM':                                  \n                    0.81024169921875,                                           \n                    'loss/test/PCAM':                                           \n                    0.43482011556625366,                                        \n                    'normalized_acc/test/PCAM'                                  \n                    : 0.9392910003662109}],                                     \n                    'FER2013':                                                  \n                    [{'acc/test/FER2013':                                       \n                    0.6565895676612854,                                         \n                    'loss/test/FER2013':                                        \n                    0.9694204330444336,                                         \n                    'normalized_acc/test/FER20                                  \n                    13': 0.9210474491119385}],                                  \n                    'OxfordIIITPet':                                            \n                    [{'acc/test/OxfordIIITPet'                                  \n                    : 0.9035159349441528,                                       \n                    'loss/test/OxfordIIITPet':                                  \n                    0.3432009518146515,                                         \n                    'normalized_acc/test/Oxfor                                  \n                    dIIITPet':                                                  \n                    0.9747133255004883}],                                       \n                    'STL10':                                                    \n                    [{'acc/test/STL10':                                         \n                    0.9762499928474426,                                         \n                    'loss/test/STL10':                                          \n                    0.0886918231844902,                                         \n                    'normalized_acc/test/STL10                                  \n                    ': 1.0007688999176025}],                                    \n                    'CIFAR100':                                                 \n                    [{'acc/test/CIFAR100':                                      \n                    0.7226999998092651,                                         \n                    'loss/test/CIFAR100':                                       \n                    1.0731322765350342,                                         \n                    'normalized_acc/test/CIFAR                                  \n                    100':                                                       \n                    0.8310717344284058}],                                       \n                    'avg': [{'acc/test/avg':                                    \n                    0.8095737227371761,                                         \n                    'normalized_acc/test/avg':                                  \n                    0.9077145755290985}]} \n\"\"\"\nimport ast\nimport re\n\n# 1️⃣ Remove newlines ONLY (do NOT collapse spaces aggressively)\nclean_str = raw_str.replace(\"\\n\", \" \")\n\n# 2️⃣ Parse safely\ndata = ast.literal_eval(clean_str)\n\ndef get_metric(metrics, prefix):\n    \"\"\"\n    Return value whose key starts with prefix\n    (ignores broken dataset names & spaces)\n    \"\"\"\n    for k, v in metrics.items():\n        if k.startswith(prefix):\n            return v\n    return None\n\n# 3️⃣ Build table\nrows = []\nfor dataset, values in data.items():\n    metrics = values[0]\n\n    acc  = get_metric(metrics, \"acc/test/\")\n    loss = get_metric(metrics, \"loss/test/\")\n    norm = get_metric(metrics, \"normalized_acc/test/\")\n\n    rows.append((\n        dataset,\n        f\"{acc:.4f}\" if acc is not None else \"—\",\n        f\"{loss:.4f}\" if loss is not None else \"—\",\n        f\"{norm:.4f}\" if norm is not None else \"—\",\n    ))\n\n# 4️⃣ Print Kaggle-ready Markdown table\nprint(\"| Dataset | Acc | Loss | Norm Acc |\")\nprint(\"|--------|----:|-----:|---------:|\")\nfor r in rows:\n    print(f\"| {r[0]} | {r[1]} | {r[2]} | {r[3]} |\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-02-02T19:22:02.479651Z","iopub.execute_input":"2026-02-02T19:22:02.479950Z","iopub.status.idle":"2026-02-02T19:22:02.491324Z","shell.execute_reply.started":"2026-02-02T19:22:02.479919Z","shell.execute_reply":"2026-02-02T19:22:02.490597Z"}},"outputs":[{"name":"stdout","text":"| Dataset | Acc | Loss | Norm Acc |\n|--------|----:|-----:|---------:|\n| SUN397 | 0.6852 | 1.1309 | 0.9152 |\n| Cars | 0.6976 | 0.9557 | 0.8739 |\n| RESISC45 | 0.8275 | 0.5888 | 0.8683 |\n| EuroSAT | 0.9219 | 0.2327 | 0.9287 |\n| SVHN | 0.7905 | 0.6651 | 0.8178 |\n| GTSRB | 0.8779 | 0.4489 | 0.8874 |\n| MNIST | 0.9758 | 0.1288 | 0.9812 |\n| DTD | 0.6766 | 1.2070 | 0.8653 |\n| Flowers102 | 0.8118 | 0.7372 | 0.9034 |\n| PCAM | 0.8102 | 0.4348 | 0.9393 |\n| FER2013 | 0.6566 | 0.9694 | 0.9210 |\n| OxfordIIITPet | 0.9035 | 0.3432 | 0.9747 |\n| STL10 | 0.9762 | 0.0887 | 1.0008 |\n| CIFAR100 | 0.7227 | 1.0731 | 0.8311 |\n| avg | 0.8096 | — | 0.9077 |\n","output_type":"stream"}],"execution_count":10}]}